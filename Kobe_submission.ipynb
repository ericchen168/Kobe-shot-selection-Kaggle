{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv')\n",
    "df['home']=df['matchup'].apply(lambda x: 1 if 'vs' in x else 0)\n",
    "df['game_month'] = pd.DatetimeIndex(df['game_date']).month\n",
    "df['game_year'] = pd.DatetimeIndex(df['game_date']).year\n",
    "df[\"time_remaining\"] = df[\"minutes_remaining\"] * 60 + df[\"seconds_remaining\"]\n",
    "df['last_5_sec'] = df['time_remaining'] < 5\n",
    "\n",
    "test_df=df.loc[(df.shot_made_flag.isnull()) ,:]\n",
    "train_df=df.loc[(df.shot_made_flag.notnull()) ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature=['action_type','game_month','game_year','loc_x', 'loc_y','combined_shot_type','minutes_remaining','time_remaining','period','last_5_sec','playoffs','season','shot_distance','shot_type','shot_zone_area','shot_zone_basic','shot_zone_range','opponent','home']\n",
    "\n",
    "\n",
    "X=train_df[feature]\n",
    "X_test=test_df[feature]\n",
    "y=train_df['shot_made_flag']\n",
    "y_test=test_df['shot_made_flag']\n",
    "feature_dum=['action_type','combined_shot_type','game_month','game_year','period','season','shot_type','shot_zone_area','shot_zone_basic','shot_zone_range','opponent']\n",
    "X=pd.get_dummies(X,columns=feature_dum,drop_first=False)\n",
    "X_test=pd.get_dummies(X_test,columns=feature_dum,drop_first=False)\n",
    "\n",
    "feature_new=[]\n",
    "for i in list(X_test.columns.values):\n",
    "    if i in list(X.columns.values):\n",
    "        feature_new.append(i)\n",
    "X=X[feature_new]\n",
    "X_test=X_test[feature_new]\n",
    "\n",
    "from sklearn import preprocessing\n",
    "normalizer = preprocessing.Normalizer()\n",
    "#X = preprocessing.MaxAbsScaler().fit_transform(X)\n",
    "#X = preprocessing.StandardScaler().fit_transform(X)\n",
    "#X_test = preprocessing.MaxAbsScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test with Random Forest\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "n_estimators_opt=list(range(10,35,5))\n",
    "oob_score_opt=['True','False']\n",
    "criterion_opt=['gini','entropy']\n",
    "param_grid=dict(n_estimators=n_estimators_opt,criterion=criterion_opt,oob_score=oob_score_opt)\n",
    "randomf = RandomForestClassifier()\n",
    "grid=GridSearchCV(randomf,param_grid,cv=10,scoring='log_loss')\n",
    "grid.fit(X,y)\n",
    "grid.grid_scores_\n",
    "\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1,\n",
    "            oob_score='False', random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "print \"Features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), forest.feature_importances_), X.columns), \n",
    "             reverse=True)\n",
    "\n",
    "# Print the feature ranking\n",
    "#print(\"Feature ranking:\")\n",
    "\n",
    "#for f in range(X.shape[1]):\n",
    "#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))   \n",
    "# Plot the feature importances of the forest\n",
    "#plt.figure()\n",
    "#plt.title(\"Feature importances\")\n",
    "#plt.bar(range(X.shape[1]), importances[indices],\n",
    "#       color=\"r\", align=\"center\")\n",
    "#plt.xticks(range(X.shape[1]), indices)\n",
    "#plt.xlim([-1, X.shape[1]])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test with GradienBoosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "n_estimators_opt=[20,50,100,500]\n",
    "depth_opt=list(range(3,6))\n",
    "param_grid=dict(n_estimators=n_estimators_opt,max_depth=depth_opt)\n",
    "clfgrad = GradientBoostingClassifier(random_state=42)\n",
    "grid=GridSearchCV(clfgrad,param_grid,cv=5,scoring='log_loss')\n",
    "grid.fit(X,y)\n",
    "grid.grid_scores_\n",
    "\n",
    "\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adaboost\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "n_estimators_opt=list(range(60,120,20))\n",
    "param_grid=dict(n_estimators=n_estimators_opt)\n",
    "clfada = AdaBoostClassifier(random_state=42)\n",
    "grid=GridSearchCV(clfada,param_grid,cv=5,scoring='log_loss')\n",
    "grid.fit(X,y)\n",
    "grid.grid_scores_\n",
    "\n",
    "\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "cv_params = {'max_depth': [7], 'min_child_weight': [1],'gamma': [0.75],'learning_rate': [0.05], 'subsample': [0.8],'n_estimators': [200,500]}\n",
    "ind_params = {'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic','eval_metric': 'logloss'}\n",
    "xgb_test=xgb.XGBClassifier((ind_params))\n",
    "#xgb_test=xgb.XGBClassifier()\n",
    "#grid=GridSearchCV(xgb_test,cv_params,cv=5,scoring='log_loss')\n",
    "grid=GridSearchCV(xgb_test,cv_params,cv=5,scoring='log_loss')\n",
    "grid.fit(X,y)\n",
    "grid.grid_scores_\n",
    "\n",
    "\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "xgb_test=xgb.XGBClassifier(objective= 'binary:logistic')\n",
    "scores = cross_val_score(xgb_test, X, y, cv=5,scoring='log_loss')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction=gsearch2.predict_proba(X_test)\n",
    "submission = pd.DataFrame()\n",
    "submission[\"shot_id\"] = test_df.shot_id\n",
    "submission[\"shot_made_flag\"]= prediction[:,0]\n",
    "submission.to_csv(\"sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.1,\n",
    " n_estimators=500,max_depth=5,\n",
    " min_child_weight=1,gamma=0,\n",
    " subsample=0.8,colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,scale_pos_weight=1,seed=27)\n",
    "\n",
    "xgb_param = xgb1.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(X, label=y)\n",
    "cvresult = xgb.cv(xgb_param,xgtrain, num_boost_round=50, nfold=5,\n",
    "            metrics='logloss', early_stopping_rounds=50)\n",
    "print (\"log loss score: {}\".format(cvresult['test-logloss-mean'].iloc[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1.fit(X,y)\n",
    "feat_imp = pd.Series(xgb1.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "def modelfit(alg, dtrain_x,dtrain_y, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain_x, label=dtrain_y)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=50, nfold=cv_folds,\n",
    "            metrics='logloss', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=500)\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain_x, dtrain_y,eval_metric='logloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain_x)\n",
    "    dtrain_predprob = alg.predict_proba(dtrain_x)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy (log loss) : %.4g\" % metrics.log_loss(dtrain_y, dtrain_predprob))\n",
    "    #print (\"AUC Score (accuracy): %f\" % metrics.accuracy_score(dtrain_y, dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=500,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV \n",
    "param_test1 = {\n",
    " 'max_depth':list(range(2,8,2)),\n",
    " 'min_child_weight':list(range(2,8,2))\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=200, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='log_loss',n_jobs=5,iid=False, cv=5)\n",
    "gsearch1.fit(X,y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: -0.71055, std: 0.09154, params: {'gamma': 0, 'n_estimators': 500, 'max_depth': 5},\n",
       "  mean: -0.71460, std: 0.09438, params: {'gamma': 0, 'n_estimators': 550, 'max_depth': 5},\n",
       "  mean: -0.71888, std: 0.09751, params: {'gamma': 0, 'n_estimators': 600, 'max_depth': 5},\n",
       "  mean: -0.72282, std: 0.09715, params: {'gamma': 0, 'n_estimators': 650, 'max_depth': 5},\n",
       "  mean: -0.73089, std: 0.09104, params: {'gamma': 0, 'n_estimators': 500, 'max_depth': 6},\n",
       "  mean: -0.73417, std: 0.09151, params: {'gamma': 0, 'n_estimators': 550, 'max_depth': 6},\n",
       "  mean: -0.73878, std: 0.09293, params: {'gamma': 0, 'n_estimators': 600, 'max_depth': 6},\n",
       "  mean: -0.74246, std: 0.09215, params: {'gamma': 0, 'n_estimators': 650, 'max_depth': 6},\n",
       "  mean: -0.74214, std: 0.09890, params: {'gamma': 0, 'n_estimators': 500, 'max_depth': 7},\n",
       "  mean: -0.74806, std: 0.10031, params: {'gamma': 0, 'n_estimators': 550, 'max_depth': 7},\n",
       "  mean: -0.75407, std: 0.10134, params: {'gamma': 0, 'n_estimators': 600, 'max_depth': 7},\n",
       "  mean: -0.75837, std: 0.10174, params: {'gamma': 0, 'n_estimators': 650, 'max_depth': 7},\n",
       "  mean: -0.75222, std: 0.10285, params: {'gamma': 0, 'n_estimators': 500, 'max_depth': 8},\n",
       "  mean: -0.75874, std: 0.10364, params: {'gamma': 0, 'n_estimators': 550, 'max_depth': 8},\n",
       "  mean: -0.76328, std: 0.10386, params: {'gamma': 0, 'n_estimators': 600, 'max_depth': 8},\n",
       "  mean: -0.76956, std: 0.10631, params: {'gamma': 0, 'n_estimators': 650, 'max_depth': 8},\n",
       "  mean: -0.75934, std: 0.10417, params: {'gamma': 0, 'n_estimators': 500, 'max_depth': 9},\n",
       "  mean: -0.76492, std: 0.10548, params: {'gamma': 0, 'n_estimators': 550, 'max_depth': 9},\n",
       "  mean: -0.77197, std: 0.10730, params: {'gamma': 0, 'n_estimators': 600, 'max_depth': 9},\n",
       "  mean: -0.77956, std: 0.10916, params: {'gamma': 0, 'n_estimators': 650, 'max_depth': 9},\n",
       "  mean: -0.71196, std: 0.09652, params: {'gamma': 0.5, 'n_estimators': 500, 'max_depth': 5},\n",
       "  mean: -0.71479, std: 0.09747, params: {'gamma': 0.5, 'n_estimators': 550, 'max_depth': 5},\n",
       "  mean: -0.71864, std: 0.09976, params: {'gamma': 0.5, 'n_estimators': 600, 'max_depth': 5},\n",
       "  mean: -0.72501, std: 0.09907, params: {'gamma': 0.5, 'n_estimators': 650, 'max_depth': 5},\n",
       "  mean: -0.72984, std: 0.09664, params: {'gamma': 0.5, 'n_estimators': 500, 'max_depth': 6},\n",
       "  mean: -0.73423, std: 0.09658, params: {'gamma': 0.5, 'n_estimators': 550, 'max_depth': 6},\n",
       "  mean: -0.73770, std: 0.09729, params: {'gamma': 0.5, 'n_estimators': 600, 'max_depth': 6},\n",
       "  mean: -0.74007, std: 0.09748, params: {'gamma': 0.5, 'n_estimators': 650, 'max_depth': 6},\n",
       "  mean: -0.73432, std: 0.09793, params: {'gamma': 0.5, 'n_estimators': 500, 'max_depth': 7},\n",
       "  mean: -0.73875, std: 0.10020, params: {'gamma': 0.5, 'n_estimators': 550, 'max_depth': 7},\n",
       "  mean: -0.74413, std: 0.09984, params: {'gamma': 0.5, 'n_estimators': 600, 'max_depth': 7},\n",
       "  mean: -0.74719, std: 0.09980, params: {'gamma': 0.5, 'n_estimators': 650, 'max_depth': 7},\n",
       "  mean: -0.74399, std: 0.09827, params: {'gamma': 0.5, 'n_estimators': 500, 'max_depth': 8},\n",
       "  mean: -0.75056, std: 0.09950, params: {'gamma': 0.5, 'n_estimators': 550, 'max_depth': 8},\n",
       "  mean: -0.75592, std: 0.10020, params: {'gamma': 0.5, 'n_estimators': 600, 'max_depth': 8},\n",
       "  mean: -0.76066, std: 0.10122, params: {'gamma': 0.5, 'n_estimators': 650, 'max_depth': 8},\n",
       "  mean: -0.76153, std: 0.10200, params: {'gamma': 0.5, 'n_estimators': 500, 'max_depth': 9},\n",
       "  mean: -0.76782, std: 0.10427, params: {'gamma': 0.5, 'n_estimators': 550, 'max_depth': 9},\n",
       "  mean: -0.77380, std: 0.10484, params: {'gamma': 0.5, 'n_estimators': 600, 'max_depth': 9},\n",
       "  mean: -0.77729, std: 0.10437, params: {'gamma': 0.5, 'n_estimators': 650, 'max_depth': 9}],\n",
       " {'gamma': 0, 'max_depth': 5, 'n_estimators': 500},\n",
       " -0.71055335863759306)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.grid_search import GridSearchCV \n",
    "param_test2 = {\n",
    " 'n_estimators':[500,550,600,650],\n",
    " 'max_depth':list(range(5,10,1)),\n",
    "'gamma':[0,0.5]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=500, max_depth=7,\n",
    " min_child_weight=4, gamma=0.5,subsample=1, colsample_bytree=0.55,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test2, scoring='log_loss',n_jobs=5,iid=False, cv=5)\n",
    "gsearch2.fit(X,y)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss score: 49    0.602802\n",
      "Name: test-logloss-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.1,\n",
    " n_estimators=500,max_depth=5,\n",
    " min_child_weight=1,gamma=0.5,\n",
    " subsample=0.8,colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,scale_pos_weight=1,seed=27)\n",
    "\n",
    "xgb_param = xgb1.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(X, label=y)\n",
    "cvresult = xgb.cv(xgb_param,xgtrain, num_boost_round=50, nfold=5,\n",
    "            metrics='logloss', early_stopping_rounds=50)\n",
    "print (\"log loss score: {}\".format(cvresult['test-logloss-mean'].iloc[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss score: 49    0.602048\n",
      "Name: test-logloss-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.1,\n",
    " n_estimators=500,max_depth=7,\n",
    " min_child_weight=1,gamma=0.5,\n",
    " subsample=0.8,colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,scale_pos_weight=1,seed=27)\n",
    "\n",
    "xgb_param = xgb1.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(X, label=y)\n",
    "cvresult = xgb.cv(xgb_param,xgtrain, num_boost_round=50, nfold=5,\n",
    "            metrics='logloss', early_stopping_rounds=50)\n",
    "print (\"log loss score: {}\".format(cvresult['test-logloss-mean'].iloc[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss score: 399    0.601774\n",
      "Name: test-logloss-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate =0.01,\n",
    " n_estimators=500,max_depth=6,\n",
    " min_child_weight=1,gamma=0.6,\n",
    " subsample=0.8,colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,scale_pos_weight=1,seed=27)\n",
    "\n",
    "xgb_param = xgb1.get_xgb_params()\n",
    "xgtrain = xgb.DMatrix(X, label=y)\n",
    "cvresult = xgb.cv(xgb_param,xgtrain, num_boost_round=400, nfold=5,\n",
    "            metrics='logloss', early_stopping_rounds=50)\n",
    "print (\"log loss score: {}\".format(cvresult['test-logloss-mean'].iloc[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.691597</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.691528</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.690074</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.689924</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.688637</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.688408</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.687222</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.686922</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.685821</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.685447</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.684412</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.683963</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.683082</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.682550</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.681727</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.681105</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.680435</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.679739</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.679111</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.678340</td>\n",
       "      <td>0.000296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.677912</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.677061</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.676746</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.675830</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.675503</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.674516</td>\n",
       "      <td>0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.674303</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.673241</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.673143</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.672006</td>\n",
       "      <td>0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.671999</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.670789</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.670867</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.669583</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.669744</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.668383</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.668737</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.667305</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.667705</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.666190</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.666680</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.665095</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.665624</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.663961</td>\n",
       "      <td>0.000531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.664603</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.662854</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.663640</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.661809</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.662632</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.660718</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.661704</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.659705</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.660866</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.658793</td>\n",
       "      <td>0.000534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.659967</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.657823</td>\n",
       "      <td>0.000623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.659088</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.656867</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.658267</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.655963</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.602022</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.578872</td>\n",
       "      <td>0.001094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.602007</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.578817</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.601995</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.578744</td>\n",
       "      <td>0.001089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.601982</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.578693</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.601972</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.578636</td>\n",
       "      <td>0.001103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.601962</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.578563</td>\n",
       "      <td>0.001090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.601954</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.578519</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.601947</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.578460</td>\n",
       "      <td>0.001084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.601934</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.578389</td>\n",
       "      <td>0.001076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.601923</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.578330</td>\n",
       "      <td>0.001083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.601921</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.578258</td>\n",
       "      <td>0.001082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.601908</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.578206</td>\n",
       "      <td>0.001089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.601895</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.578175</td>\n",
       "      <td>0.001094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.601895</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.578115</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.601887</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.578041</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.601886</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.578002</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.601880</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.577939</td>\n",
       "      <td>0.001055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.601872</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.577874</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.601863</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.577836</td>\n",
       "      <td>0.001038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.601854</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.577792</td>\n",
       "      <td>0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.601850</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.577751</td>\n",
       "      <td>0.001043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.601847</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.577692</td>\n",
       "      <td>0.001041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.601836</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.577645</td>\n",
       "      <td>0.001041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.601819</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.577574</td>\n",
       "      <td>0.001036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.601808</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.577502</td>\n",
       "      <td>0.001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.601806</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.577430</td>\n",
       "      <td>0.001022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.601797</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.577382</td>\n",
       "      <td>0.001014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.601796</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.577311</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.601786</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.577259</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.601774</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.577210</td>\n",
       "      <td>0.001051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-logloss-mean  test-logloss-std  train-logloss-mean  \\\n",
       "0             0.691597          0.000043            0.691528   \n",
       "1             0.690074          0.000105            0.689924   \n",
       "2             0.688637          0.000113            0.688408   \n",
       "3             0.687222          0.000275            0.686922   \n",
       "4             0.685821          0.000344            0.685447   \n",
       "5             0.684412          0.000395            0.683963   \n",
       "6             0.683082          0.000379            0.682550   \n",
       "7             0.681727          0.000391            0.681105   \n",
       "8             0.680435          0.000439            0.679739   \n",
       "9             0.679111          0.000461            0.678340   \n",
       "10            0.677912          0.000527            0.677061   \n",
       "11            0.676746          0.000517            0.675830   \n",
       "12            0.675503          0.000514            0.674516   \n",
       "13            0.674303          0.000525            0.673241   \n",
       "14            0.673143          0.000575            0.672006   \n",
       "15            0.671999          0.000580            0.670789   \n",
       "16            0.670867          0.000576            0.669583   \n",
       "17            0.669744          0.000583            0.668383   \n",
       "18            0.668737          0.000649            0.667305   \n",
       "19            0.667705          0.000677            0.666190   \n",
       "20            0.666680          0.000763            0.665095   \n",
       "21            0.665624          0.000774            0.663961   \n",
       "22            0.664603          0.000801            0.662854   \n",
       "23            0.663640          0.000873            0.661809   \n",
       "24            0.662632          0.000884            0.660718   \n",
       "25            0.661704          0.000902            0.659705   \n",
       "26            0.660866          0.000946            0.658793   \n",
       "27            0.659967          0.000944            0.657823   \n",
       "28            0.659088          0.000970            0.656867   \n",
       "29            0.658267          0.000993            0.655963   \n",
       "..                 ...               ...                 ...   \n",
       "370           0.602022          0.003311            0.578872   \n",
       "371           0.602007          0.003304            0.578817   \n",
       "372           0.601995          0.003296            0.578744   \n",
       "373           0.601982          0.003296            0.578693   \n",
       "374           0.601972          0.003295            0.578636   \n",
       "375           0.601962          0.003296            0.578563   \n",
       "376           0.601954          0.003295            0.578519   \n",
       "377           0.601947          0.003293            0.578460   \n",
       "378           0.601934          0.003295            0.578389   \n",
       "379           0.601923          0.003299            0.578330   \n",
       "380           0.601921          0.003310            0.578258   \n",
       "381           0.601908          0.003312            0.578206   \n",
       "382           0.601895          0.003311            0.578175   \n",
       "383           0.601895          0.003310            0.578115   \n",
       "384           0.601887          0.003319            0.578041   \n",
       "385           0.601886          0.003315            0.578002   \n",
       "386           0.601880          0.003312            0.577939   \n",
       "387           0.601872          0.003311            0.577874   \n",
       "388           0.601863          0.003317            0.577836   \n",
       "389           0.601854          0.003319            0.577792   \n",
       "390           0.601850          0.003320            0.577751   \n",
       "391           0.601847          0.003320            0.577692   \n",
       "392           0.601836          0.003325            0.577645   \n",
       "393           0.601819          0.003328            0.577574   \n",
       "394           0.601808          0.003329            0.577502   \n",
       "395           0.601806          0.003329            0.577430   \n",
       "396           0.601797          0.003329            0.577382   \n",
       "397           0.601796          0.003339            0.577311   \n",
       "398           0.601786          0.003348            0.577259   \n",
       "399           0.601774          0.003357            0.577210   \n",
       "\n",
       "     train-logloss-std  \n",
       "0             0.000016  \n",
       "1             0.000020  \n",
       "2             0.000086  \n",
       "3             0.000136  \n",
       "4             0.000185  \n",
       "5             0.000194  \n",
       "6             0.000240  \n",
       "7             0.000256  \n",
       "8             0.000277  \n",
       "9             0.000296  \n",
       "10            0.000360  \n",
       "11            0.000409  \n",
       "12            0.000402  \n",
       "13            0.000467  \n",
       "14            0.000455  \n",
       "15            0.000440  \n",
       "16            0.000483  \n",
       "17            0.000490  \n",
       "18            0.000490  \n",
       "19            0.000511  \n",
       "20            0.000556  \n",
       "21            0.000531  \n",
       "22            0.000541  \n",
       "23            0.000493  \n",
       "24            0.000497  \n",
       "25            0.000473  \n",
       "26            0.000534  \n",
       "27            0.000623  \n",
       "28            0.000589  \n",
       "29            0.000673  \n",
       "..                 ...  \n",
       "370           0.001094  \n",
       "371           0.001102  \n",
       "372           0.001089  \n",
       "373           0.001099  \n",
       "374           0.001103  \n",
       "375           0.001090  \n",
       "376           0.001100  \n",
       "377           0.001084  \n",
       "378           0.001076  \n",
       "379           0.001083  \n",
       "380           0.001082  \n",
       "381           0.001089  \n",
       "382           0.001094  \n",
       "383           0.001091  \n",
       "384           0.001073  \n",
       "385           0.001062  \n",
       "386           0.001055  \n",
       "387           0.001044  \n",
       "388           0.001038  \n",
       "389           0.001037  \n",
       "390           0.001043  \n",
       "391           0.001041  \n",
       "392           0.001041  \n",
       "393           0.001036  \n",
       "394           0.001011  \n",
       "395           0.001022  \n",
       "396           0.001014  \n",
       "397           0.001026  \n",
       "398           0.001044  \n",
       "399           0.001051  \n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fitting(gamm):\n",
    "    import xgboost as xgb\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "    from sklearn import cross_validation, metrics\n",
    "\n",
    "    xgb1 = XGBClassifier(learning_rate =0.1,\n",
    "     n_estimators=500,max_depth=6,\n",
    "     min_child_weight=1,gamma=gamm,\n",
    "     subsample=0.8,colsample_bytree=0.8,\n",
    "     objective= 'binary:logistic',\n",
    "     nthread=4,scale_pos_weight=1,seed=27)\n",
    "\n",
    "    xgb_param = xgb1.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(X, label=y)\n",
    "    cvresult = xgb.cv(xgb_param,xgtrain, num_boost_round=50, nfold=5,\n",
    "            metrics='logloss', early_stopping_rounds=50)\n",
    "    return (cvresult['test-logloss-mean'][49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60204360000000001, 0.60197319999999999, 0.60205580000000003, 0.6020629999999999, 0.60199419999999992, 0.60188780000000008, 0.60214299999999998, 0.60208939999999989, 0.60199999999999998, 0.60192799999999991]\n"
     ]
    }
   ],
   "source": [
    "n_estimator=[400,450,500,550,600]\n",
    "max_depth=[4,5,6,7,8,9]\n",
    "gamm=[x / 100 for x in list(range(50,70,2))]\n",
    "min_c=[1,2,3,4,5,6,7]\n",
    "learn=[0.1,0.05,0.02,0.01]\n",
    "score=[]\n",
    "for n in gamm:\n",
    "    score.append(fitting(n))\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1.bst = xgb.train(xgb_param, xgtrain, num_boost_round=400)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "test_y = xgb1.bst.predict(dtest)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission[\"shot_id\"] = test_df.shot_id\n",
    "submission[\"shot_made_flag\"]= test_y\n",
    "submission.to_csv(\"sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
